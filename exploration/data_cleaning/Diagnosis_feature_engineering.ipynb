{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking the table with diagnosis information\n",
    "### Diagnosis information is contained in multiple tables\n",
    "* The ADMISSIONS.csv table has a brief diagnosis description for which the patient has been admitted, it is written as free text and does not conform to systematic ontology (ICD9-International Classification of Diseases).\n",
    "* DIAGNOSES_ICD.csv table contains ICD9 codes of all the diagnosis assigned to a particular hospital stay.\n",
    "* A short and long description of ICD9 codes is in the D_ICD_DIAGNOSES.csv table.\n",
    "* Finally, a higher level description is in DRGCODES.csv table. DRG (Diagnosis Related Group) type (DRG_TYPE) together with DRG_CODE identify a unique group.\n",
    "    * Note, DRG_CODE alone does not uniquely identify a diagnoses, since different diagnosis types can have the same code.\n",
    "    * Diagnosis description also contains important information on comorbid conditions or compliclations.\n",
    "    * This table also contains DRG_SEVERITY and DRG_MORTALITY scores, which score the severity and mortality of the given diagnosis on the scale of 0-4. This is an exptremely useful metric, however, only APR (all payer regitry) DRG_TYPE has a value assigned.\n",
    "    \n",
    "I will engineer features from the frequent words in the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125557 entries, 0 to 125556\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   ROW_ID         125557 non-null  int64  \n",
      " 1   SUBJECT_ID     125557 non-null  int64  \n",
      " 2   HADM_ID        125557 non-null  int64  \n",
      " 3   DRG_TYPE       125557 non-null  object \n",
      " 4   DRG_CODE       125557 non-null  int64  \n",
      " 5   DESCRIPTION    125494 non-null  object \n",
      " 6   DRG_SEVERITY   66634 non-null   float64\n",
      " 7   DRG_MORTALITY  66634 non-null   float64\n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 7.7+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# load the drgcodes tables \n",
    "diagnoses_df = pd.read_csv('../../data/raw/DRGCODES.csv')\n",
    "diagnoses_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the original dataframe is  125557\n",
      "After grouping by admission the number of rows is  58890\n",
      "The number of qualifying admissions without a diagnosis description  0\n",
      "Number of diagnosis severity missing entries is  58923\n",
      "Number of diagnosis mortality missing entries is  58923\n",
      "Severity score forward filling complete\n",
      "Time elapsed in seconds  33.18370428399999\n",
      "Severity score backward filling complete\n",
      "Time elapsed in seconds  20.87144844999989\n",
      "Mortality score forward filling complete\n",
      "Time elapsed in seconds  20.603132451000192\n",
      "Mortality score backward filling complete\n",
      "Time elapsed in seconds  20.57246440100016\n",
      "After tranformation number of diagnosis severity missing entries is  19506\n",
      "After tranformation number of diagnosis mortality missing entries is  19506\n",
      "Time elapsed in seconds  0.0025895280000440835\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.monotonic()\n",
    "\n",
    "# get rid of unnecessary columns to free up memory\n",
    "diagnoses_df.drop(columns=['ROW_ID', 'SUBJECT_ID', 'DRG_TYPE', 'DRG_CODE'], inplace=True)\n",
    "\n",
    "# Combine multiple descriptions per admission\n",
    "print(\"The length of the original dataframe is \", len(diagnoses_df))\n",
    "desc_combined = diagnoses_df.groupby('HADM_ID')['DESCRIPTION'].apply(lambda x: x.str.cat(sep=', '))\n",
    "\n",
    "print(\"After grouping by admission the number of rows is \", len(desc_combined))\n",
    "print(\"The number of qualifying admissions without a diagnosis description \", desc_combined.isnull().sum())\n",
    "\n",
    "# Fill out severity and mortality scores to be the same per group.\n",
    "# before filling out find out the number of missing values\n",
    "print(\"Number of diagnosis severity missing entries is \", diagnoses_df.DRG_SEVERITY.isnull().sum())\n",
    "print(\"Number of diagnosis mortality missing entries is \", diagnoses_df.DRG_MORTALITY.isnull().sum())\n",
    "\n",
    "# use forward fill and backward fill to fill out missing values\n",
    "diagnoses_df['DRG_SEVERITY'] = diagnoses_df.groupby('HADM_ID')['DRG_SEVERITY'].transform(lambda x: x.fillna(method='ffill'))\n",
    "print(\"Severity score forward filling complete\")\n",
    "print(\"Time elapsed in seconds \", time.monotonic()-start_time)\n",
    "start_time = time.monotonic()\n",
    "\n",
    "diagnoses_df['DRG_SEVERITY'] = diagnoses_df.groupby('HADM_ID')['DRG_SEVERITY'].transform(lambda x: x.fillna(method='bfill'))  \n",
    "print(\"Severity score backward filling complete\")\n",
    "print(\"Time elapsed in seconds \", time.monotonic()-start_time)\n",
    "start_time = time.monotonic()\n",
    "\n",
    "diagnoses_df['DRG_MORTALITY'] = diagnoses_df.groupby('HADM_ID')['DRG_MORTALITY'].transform(lambda x: x.fillna(method='ffill'))\n",
    "print(\"Mortality score forward filling complete\")\n",
    "print(\"Time elapsed in seconds \", time.monotonic()-start_time)\n",
    "start_time = time.monotonic()\n",
    "\n",
    "diagnoses_df['DRG_MORTALITY'] = diagnoses_df.groupby('HADM_ID')['DRG_MORTALITY'].transform(lambda x: x.fillna(method='bfill')) \n",
    "print(\"Mortality score backward filling complete\")\n",
    "print(\"Time elapsed in seconds \", time.monotonic()-start_time)\n",
    "start_time = time.monotonic()\n",
    "\n",
    "print(\"After tranformation number of diagnosis severity missing entries is \", diagnoses_df.DRG_SEVERITY.isnull().sum())\n",
    "print(\"After tranformation number of diagnosis mortality missing entries is \", diagnoses_df.DRG_MORTALITY.isnull().sum())\n",
    "print(\"Time elapsed in seconds \", time.monotonic()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing severity scores in the unique admission dataset 19495\n",
      "Number of missing mortality scores in the unique admission dataset 19495\n",
      "After median value fill the number of missing severity scores in the unique admission dataset 0\n",
      "After median value fill the nNumber of missing mortality scores in the unique admission dataset 0\n"
     ]
    }
   ],
   "source": [
    "# The remaining missing values will be filled with the mean of the column, but first need to get rid of \n",
    "# redundant entries\n",
    "sev_combined = diagnoses_df.groupby('HADM_ID')['DRG_SEVERITY'].mean()\n",
    "mort_combined = diagnoses_df.groupby('HADM_ID')['DRG_MORTALITY'].mean()\n",
    "\n",
    "print(\"Number of missing severity scores in the unique admission dataset\", sev_combined.isnull().sum())\n",
    "print(\"Number of missing mortality scores in the unique admission dataset\", mort_combined.isnull().sum())\n",
    "\n",
    "sev_combined = sev_combined.fillna(sev_combined.median())\n",
    "mort_combined = mort_combined.fillna(mort_combined.median())\n",
    "\n",
    "print(\"After median value fill the number of missing severity scores in the unique admission dataset\", sev_combined.isnull().sum())\n",
    "print(\"After median value fill the nNumber of missing mortality scores in the unique admission dataset\", mort_combined.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 58890 entries, 0 to 58889\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   HADM_ID        58890 non-null  int64  \n",
      " 1   DESCRIPTION    58890 non-null  object \n",
      " 2   DRG_SEVERITY   58890 non-null  float64\n",
      " 3   DRG_MORTALITY  58890 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#Finally merge all the grouped data frames and the output labels\n",
    "desc_combined = desc_combined.reset_index()\n",
    "sev_combined = sev_combined.reset_index()\n",
    "mort_combined = mort_combined.reset_index()\n",
    "desc_combined = pd.merge(desc_combined, sev_combined[['HADM_ID', 'DRG_SEVERITY']], left_on = 'HADM_ID', right_on='HADM_ID', how='left')\n",
    "desc_combined = pd.merge(desc_combined, mort_combined[['HADM_ID', 'DRG_MORTALITY']], left_on = 'HADM_ID', right_on='HADM_ID', how='left')\n",
    "desc_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.4.5.zip (1.5 MB)\n",
      "Requirement already satisfied: six in /Users/zhannahakhverdyan/.virtualenvs/MIMIC-III-readmission-prediction/lib/python3.7/site-packages (from nltk) (1.14.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449905 sha256=9530c8da075bd308605e94e09a87ebc153dd0c115e317ae63d0d57fc37721160\n",
      "  Stored in directory: /Users/zhannahakhverdyan/Library/Caches/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words in the descriptions\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "import string\n",
    "\n",
    "def parseOutText(all_text):\n",
    "    \"\"\" given text field parse out all text\n",
    "        and return a string that contains all the words\n",
    "        \"\"\"\n",
    "    words = \"\"\n",
    "    if len(all_text) > 1:        \n",
    "        text_string = all_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        spl = text_string.split()\n",
    "        for i in spl:\n",
    "            i = stemmer.stem(i)\n",
    "            words += i + ' '\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           diabet w cc diabet diabet \n",
       "1    peptic ulcer gastriti peptic ulcer gastriti gi...\n",
       "2                   chronic obstruct pulmonari diseas \n",
       "3    major small larg bowel procedur w cc w major g...\n",
       "4    coronari bypass wo cardiac cath or percutan ca...\n",
       "Name: DESCRIPTION, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rid of any numbers in diagnosis description\n",
    "desc_combined['DESCRIPTION'] = desc_combined['DESCRIPTION'].str.replace('\\d+', '')\n",
    "\n",
    "# process the text in descriptions\n",
    "desc_combined['DESCRIPTION'] = desc_combined['DESCRIPTION'].apply(parseOutText)\n",
    "desc_combined['DESCRIPTION'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# text vectorization: go from text to word count arrays\n",
    "# ignore the words appearing in more than 50% of documents and select top 200 words\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words=stopwords, max_features=200)\n",
    "# transform descriptions\n",
    "description_transformed = vectorizer.fit_transform(desc_combined['DESCRIPTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abdomin</th>\n",
       "      <th>abus</th>\n",
       "      <th>acut</th>\n",
       "      <th>age</th>\n",
       "      <th>agent</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>aliv</th>\n",
       "      <th>ami</th>\n",
       "      <th>anomali</th>\n",
       "      <th>anoth</th>\n",
       "      <th>...</th>\n",
       "      <th>treatment</th>\n",
       "      <th>ulcer</th>\n",
       "      <th>unrel</th>\n",
       "      <th>unspecifi</th>\n",
       "      <th>urinari</th>\n",
       "      <th>valv</th>\n",
       "      <th>vascular</th>\n",
       "      <th>ventil</th>\n",
       "      <th>without</th>\n",
       "      <th>wo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.359093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abdomin  abus  acut  age  agent  alcohol  aliv  ami  anomali  anoth  ...  \\\n",
       "0      0.0   0.0   0.0  0.0    0.0      0.0   0.0  0.0      0.0    0.0  ...   \n",
       "1      0.0   0.0   0.0  0.0    0.0      0.0   0.0  0.0      0.0    0.0  ...   \n",
       "2      0.0   0.0   0.0  0.0    0.0      0.0   0.0  0.0      0.0    0.0  ...   \n",
       "3      0.0   0.0   0.0  0.0    0.0      0.0   0.0  0.0      0.0    0.0  ...   \n",
       "4      0.0   0.0   0.0  0.0    0.0      0.0   0.0  0.0      0.0    0.0  ...   \n",
       "\n",
       "   treatment     ulcer  unrel  unspecifi  urinari  valv  vascular  ventil  \\\n",
       "0        0.0  0.000000    0.0        0.0      0.0   0.0       0.0     0.0   \n",
       "1        0.0  0.522269    0.0        0.0      0.0   0.0       0.0     0.0   \n",
       "2        0.0  0.000000    0.0        0.0      0.0   0.0       0.0     0.0   \n",
       "3        0.0  0.000000    0.0        0.0      0.0   0.0       0.0     0.0   \n",
       "4        0.0  0.000000    0.0        0.0      0.0   0.0       0.0     0.0   \n",
       "\n",
       "   without        wo  \n",
       "0      0.0  0.000000  \n",
       "1      0.0  0.000000  \n",
       "2      0.0  0.000000  \n",
       "3      0.0  0.000000  \n",
       "4      0.0  0.359093  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "final_feature_df = pd.DataFrame(data=description_transformed.toarray(), columns=features)\n",
    "final_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58890, 204)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the severity, mortality scores and description featues\n",
    "columns1 = desc_combined.columns.tolist()\n",
    "columns2 = final_feature_df.columns.tolist()\n",
    "col_combined = columns1+columns2\n",
    "desc_combined = pd.concat([desc_combined, final_feature_df], axis=1, ignore_index=True)\n",
    "desc_combined.columns = col_combined\n",
    "desc_combined.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the intermediate analysis\n",
    "desc_combined.to_csv('../../data/intermediate/inter022120')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIMIC-III-readmission-prediction",
   "language": "python",
   "name": "mimic-iii-readmission-prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
